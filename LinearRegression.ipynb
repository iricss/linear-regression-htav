{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Lets assume that we have $N$ observation and $M$ features. The problem of linear regression is defined as:\n",
    "\n",
    "$$\\vec{y}_p = \\boldsymbol{X} \\vec{w},$$\n",
    "* $\\vec{y}_p$, a vector of size $N$, represents our predictions. \n",
    "* $\\boldsymbol{X}$ is a matrix of $(N\\times M)$. \n",
    "* $\\vec{w}$ is the fitting parameters, its our job to find them.\n",
    "\n",
    "Let measure how good are predictions are, we use Mean Squared Error (MSE) to calculate the distance of our predictions from true values:\n",
    "\n",
    "$$J = (\\vec{y}_p - \\vec{y}_t)^2$$\n",
    "\n",
    "* $\\vec{y}_t$ is a vector of true values. \n",
    "* $J$ is MSE and it is an scalar.\n",
    "\n",
    "Now our job is to find $\\vec{w}$ in such a way that it minimizes the cost function. Here we are going to use Stochastic Gradient Descent (SGD) to do that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_t, y_p):\n",
    "         \n",
    "    error = y_p - y_t\n",
    "    J = 0\n",
    "    for i in range(len(error)):\n",
    "        J += error[i]**2\n",
    "    return(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_mse(X, y_t, y_p):\n",
    "    \n",
    "    import numpy as np\n",
    "    XT = np.transpose(X)\n",
    "    error = y_p - y_t    \n",
    "    dJ = 2*np.dot(XT , error)\n",
    "    \n",
    "    return(dJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(w):\n",
    "    '''\n",
    "    Return l2 penalty. \n",
    "    w : weights of the model, a numpy vector of (n_features)\n",
    "    '''\n",
    "\n",
    "def d_l2(w):\n",
    "    '''\n",
    "    Return gradient of l2 penalty.\n",
    "    w : weights of the model, a numpy vector of (n_features)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(X, y_t, y_p, eta=0.1, alpha=1.0, b = 100, epoches=1000, tol=0.0001):\n",
    "    \n",
    "    import numpy as np\n",
    "    W = np.dot((np.linalg.pinv(X)),y_p)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(epoches):\n",
    "        if mse(y_t, y_p) > tol:\n",
    "            W = W - alpha* eta* d_mse(X, y_t, y_p)\n",
    "            y_p = np.dot(X, W)\n",
    "            count += 1\n",
    "            \n",
    "    print('W = ' , W)\n",
    "    print('J = ', mse(y_t, y_p))\n",
    "    print('number of iterations: ',count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(X, y_t, y_p, eta=0.0001, alpha=1.0, b = 100, epoches=1000, tol=0.00001):\n",
    "    '''\n",
    "    mini-batch Stochastic Gradient Descent learning. \n",
    "    \n",
    "    The default loss function is MSE and the default penalty is l2. \n",
    "    \n",
    "    If the test set is provided, it keeps running until the cost stop decreasing. \n",
    "    If the test set is not provided, it keeps running until the improvment in the cost is less than **tol**.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta: float \n",
    "        Learning rate, default to 0.0001.\n",
    "        \n",
    "    alpha: float\n",
    "        Regularization parameter, defaults to 1.0\n",
    "        \n",
    "    b: int \n",
    "        batch size, defaults to 100.\n",
    "    \n",
    "    epoches: int\n",
    "        number of epoches to train, defaults to 1000.\n",
    "    \n",
    "    normalize: bool\n",
    "        True: normalize data by mean and std (default).\n",
    "        False: do not change the data.\n",
    "        \n",
    "    tol: float \n",
    "        Stop training if the improvment in cost function is less than tol (when test set is not provided). \n",
    "        Defaults to 0.00001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape (n_features,)\n",
    "        Returns the weights (fitting parameters).\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    ... (write your test heere.)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a simple linear model $y =  x + 1$. Given x, this gives us the true value of $y$, ($y_t$). \n",
    "Generate some $x$ and $y$ pairs, use your code to infer fitting parameters. Plot the ground truth $y = x +1$ and your fitted model for the interval of $x \\in [-2,2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate some other $x$ and $y$ pairs, add a normally distributed noise ($\\mu =0 $ $\\sigma = 0.1$) to $y$ and infer fitting parameters again. Plot the theory and fitted line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
